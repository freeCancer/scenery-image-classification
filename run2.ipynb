{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "from skimage.io import imread\n",
    "from skimage.feature import hog\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_label():\n",
    "\n",
    "    def __init__(self, img=None,number=None,label=None):\n",
    "        self.img = img\n",
    "        self.number=number\n",
    "        self.label=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "categories = ['bedroom', 'Coast', 'Forest', 'Highway', 'industrial',\n",
    "           'Insidecity', 'kitchen', 'livingroom', 'Mountain', 'Office',\n",
    "           'OpenCountry', 'store', 'Street', 'Suburb', 'TallBuilding'] \n",
    "\n",
    "train_img_set=[]\n",
    "test_img_set=[]\n",
    "\n",
    "for j in range(15):\n",
    "    for i in range(100):\n",
    "        img_PIL = Image.open(\"E:\\\\Course\\\\Computer Vision\\\\Coursework3\\\\training\\\\training\\\\\"+ categories[j] +\"\\\\\"+str(i)+\".jpg\")\n",
    "        #img_PIL = np.array(img_PIL)\n",
    "        img=image_label(img_PIL,i,j)\n",
    "        train_img_set.append(img)\n",
    "error=0\n",
    "\n",
    "for i in range(2988):\n",
    "    try:\n",
    "        img_PIL = Image.open(\"E:\\\\Course\\\\Computer Vision\\\\Coursework3\\\\testing\\\\\"+str(i)+\".jpg\")\n",
    "        img=image_label(img_PIL,i)\n",
    "        test_img_set.append(img)\n",
    "        error=error+1\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    continue\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def patches_feature(image_set):\n",
    "    \n",
    "    rng = np.random.RandomState(0)\n",
    "    kmeans = MiniBatchKMeans(n_clusters = 200, random_state = rng)\n",
    "    \n",
    "    for i in tqdm(range(len(image_set))):\n",
    "        image=np.array(image_set[i].img)\n",
    "        row,col=image.shape\n",
    "        feature_vectors_images = []\n",
    "        for m in range(0,(row-8),4):\n",
    "            for n in range(0,(col-8),4):\n",
    "                patch_array = image[m:m+8,n:n+8].reshape(1,64)[0]\n",
    "                patch_array=(patch_array - np.mean(patch_array))/np.std(patch_array)\n",
    "                np.nan_to_num(patch_array)\n",
    "                #print(patch_array)\n",
    "                feature_vectors_images.append(patch_array)\n",
    "        feature_vectors_images=np.array(feature_vectors_images)\n",
    "        feature_vectors_images=np.nan_to_num(feature_vectors_images)\n",
    "        kmeans.partial_fit(feature_vectors_images)\n",
    "\n",
    "    \n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vocab = patches_feature(train_img_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def get_bags_of_words(image_set):\n",
    "    vocab = np.load('vocab.npy')\n",
    "    vocab_length = vocab.shape[0]\n",
    "    images_histograms = np.zeros((len(image_set), vocab_length))\n",
    "    feature_vectors_images = []\n",
    "    for i, image in enumerate(image_set):\n",
    "        image=np.array(image_set[i].img)\n",
    "        row,col=image.shape\n",
    "        feature_vectors_images = []\n",
    "        for m in range(0,(row-8),4):\n",
    "            for n in range(0,(col-8),4):\n",
    "                patch_array = image[m:m+8,n:n+8].reshape(1,64)[0]\n",
    "                patch_array=(patch_array - np.mean(patch_array))/np.std(patch_array)\n",
    "                np.nan_to_num(patch_array)\n",
    "                feature_vectors_images.append(patch_array)\n",
    "        feature_vectors_images=np.array(feature_vectors_images)\n",
    "        feature_vectors_images=np.nan_to_num(feature_vectors_images)\n",
    "        feature_vectors_images=np.array(feature_vectors_images)\n",
    "        histogram = np.zeros(vocab_length)\n",
    "        distances = cdist(feature_vectors_images, vocab)  #每一张图片和词袋进行对比\n",
    "        closest_vocab = np.argsort(distances, axis=1)[:,0]\n",
    "        indices, counts = np.unique(closest_vocab, return_counts=True)\n",
    "        histogram[indices] += counts\n",
    "        histogram = histogram / norm(histogram)\n",
    "        images_histograms[i] = histogram\n",
    "    return images_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c19ed22af4bd>:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  patch_array=(patch_array - np.mean(patch_array))/np.std(patch_array)\n"
     ]
    }
   ],
   "source": [
    "train_feature = get_bags_of_words(train_img_set)\n",
    "test_feature = get_bags_of_words(test_img_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "num_categories = len(categories)\n",
    "num_train_per_cat = 100\n",
    "\n",
    "train_labels = [None] * (num_categories * num_train_per_cat) # 1500 * 1 \n",
    "\n",
    "for i,cat in enumerate(categories):\n",
    "    for j in range(num_train_per_cat):\n",
    "        train_labels[i * num_train_per_cat + j] = cat\n",
    "\n",
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "    clf.fit(train_image_feats, train_labels)\n",
    "    test_predictions = clf.predict(test_image_feats)\n",
    "    return test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = svm_classify(train_feature, train_labels, test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Street' 'Forest' 'Insidecity' ... 'Office' 'Office' 'industrial']\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\run2.txt','a') as f:\n",
    "    for t,category in enumerate(test_predictions):\n",
    "        text =str(test_img_set[t].number) +'.jpg'+' '+ category +  '\\n'\n",
    "        f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
